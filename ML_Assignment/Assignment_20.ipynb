{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 20 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is the underlying concept of Support Vector Machines ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34729046",
   "metadata": {},
   "source": [
    "The underlying concept of Support Vector Machines is to find an optimal hyperplane that separates data points belonging to different classes with the largest possible margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7b03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is the concept of a support vector ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa4fb0",
   "metadata": {},
   "source": [
    "A support vector is a data point that lies closest to the decision boundary (hyperplane) in SVM and plays a crucial role in determining the position and orientation of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745a587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. When using SVMs, why is it necessary to scale the inputs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd495f",
   "metadata": {},
   "source": [
    "Scaling the inputs is necessary when using SVMs to ensure that features with larger scales do not dominate the optimization process, leading to biased results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8db91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4273166",
   "metadata": {},
   "source": [
    "Yes, an SVM classifier can output a confidence score or a margin distance, but it does not directly provide a percentage chance or probability estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ae9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f4399",
   "metadata": {},
   "source": [
    "When training a model on a large dataset with millions of instances and hundreds of features, it is generally more efficient to use the dual form of the SVM problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b695efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336d521",
   "metadata": {},
   "source": [
    "If an SVM classifier with an RBF kernel underfits the training collection, it is better to increase the value of gamma to make the decision boundary more flexible. As for the letter C, reducing its value can help reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae674f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e624922",
   "metadata": {},
   "source": [
    "The QP parameters (H, f, A, and b) for solving the soft margin linear SVM classifier problem with an off-the-shelf QP solver should be set according to the specific formulation of the SVM problem and the constraints imposed by the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ac2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f716511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a linearly separable dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a LinearSVC\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "# Train an SVC\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Train an SGDClassifier\n",
    "sgd = SGDClassifier(loss='hinge')\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "linear_svc_pred = linear_svc.predict(X_test)\n",
    "svc_pred = svc.predict(X_test)\n",
    "sgd_pred = sgd.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "linear_svc_accuracy = accuracy_score(y_test, linear_svc_pred)\n",
    "svc_accuracy = accuracy_score(y_test, svc_pred)\n",
    "sgd_accuracy = accuracy_score(y_test, sgd_pred)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"LinearSVC accuracy:\", linear_svc_accuracy)\n",
    "print(\"SVC accuracy:\", svc_accuracy)\n",
    "print(\"SGDClassifier accuracy:\", sgd_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d926bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM classifier with one-versus-the-rest strategy\n",
    "svm_classifier = SVC(kernel='rbf', decision_function_shape='ovr')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate precision (accuracy) on the test set\n",
    "precision = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print precision\n",
    "print(\"Precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. On the California housing dataset, train an SVM regressor ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc128984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM regressor\n",
    "svm_regressor = SVR()\n",
    "svm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_regressor.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error (MSE) on the test set\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print MSE\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
