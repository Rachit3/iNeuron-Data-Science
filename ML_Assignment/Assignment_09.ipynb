{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d4a0d",
   "metadata": {},
   "source": [
    "**Ans:** Feature engineering is the process of transforming raw data into a format that is suitable for machine learning algorithms. It involves creating new features, selecting relevant features, and transforming existing features to enhance model performance. Some aspects of feature engineering include handling missing data, encoding categorical variables, scaling numerical features, and creating interaction or polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2906477",
   "metadata": {},
   "source": [
    "**Ans:** Feature selection is the process of selecting a subset of relevant features from a larger set of available features. The aim is to reduce dimensionality, improve model interpretability, and enhance model performance. Various methods of feature selection include filter methods (e.g., correlation, mutual information), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, random forests)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c861e",
   "metadata": {},
   "source": [
    "**Ans:** The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a  subset of feature by actually training a model on it.\n",
    "      \n",
    "The filter method has the fastest running time; however, it does not consider feature dependencies and tends to each feature separately when univariate techniques are used. The wrapper method has the advantages of better  generalization and robust interaction with the classifier used for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. Please Answer the following Questions :\n",
    "1. Describe the overall feature selection process.\n",
    "2. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b9aad",
   "metadata": {},
   "source": [
    "**Ans:** Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99edb141",
   "metadata": {},
   "source": [
    "**Ans:** In the context of text categorization, feature engineering involves transforming textual data into numerical representations that machine learning algorithms can process. This includes techniques like tokenization, removing stop words, stemming/lemmatization, and creating numerical features such as TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings like Word2Vec or GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4999ef6",
   "metadata": {},
   "source": [
    "**Ans:** Cosine similarity is a good metric for text categorization because it measures the cosine of the angle between two vectors. In text categorization, documents are often represented as vectors in a high-dimensional space, where each dimension corresponds to a unique word. Cosine similarity captures the similarity in the orientation of these vectors, irrespective of their magnitude. To find the resemblance in cosine for the given document-term matrix, you calculate the cosine similarity between the two rows: (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e636eaf",
   "metadata": {},
   "source": [
    "##### 7. Explain the following:\n",
    "1. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "2. Compare the Jaccard index and similarity matching coefficient of two features with values `(1,1,0,0,1,0,1,1)` and `(1,1,0,0, 0,1,1,1)`, respectively `(1,0,0,1,1,0,0,1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ea32c",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "\n",
    " i. The formula for calculating Hamming distance is the number of positions at which two strings of equal length differ. For example, between \"10001011\" and \"11001111\", the Hamming distance is 3 because they differ in the 3rd, 5th, and 8th positions.\n",
    "\n",
    "ii. The Jaccard index measures the similarity between two sets and is calculated as the ratio of the intersection to the union of the sets. The similarity matching coefficient compares the similarity of two sets by counting the number of matching elements. For the given features (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), and (1, 0, 0, 1, 1, 0, 0, 1), the Jaccard index is 0.5 (2 matching elements out of 4 total elements), and the similarity matching coefficient is 0.75 (3 matching elements out of 4 total elements)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64feb7de",
   "metadata": {},
   "source": [
    "##### 8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da31e0f",
   "metadata": {},
   "source": [
    "**Ans:** High dimension is when variable numbers p is higher than the sample sizes n i.e. p>n, cases. High dimensional data is referred to a data of n samples with p features, where p is larger than n.\n",
    "\n",
    "For example, tomographic imaging data, ECG data, and MEG data. One example of high dimensional data is microarray gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec9c51",
   "metadata": {},
   "source": [
    "##### 9. Make a few quick notes on:\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "2. Use of vectors\n",
    "3. Embedded technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821bb5b",
   "metadata": {},
   "source": [
    "**Ans:** The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data. The technique is widely used to emphasize variation  and capture strong patterns in a data set.\n",
    "\n",
    "Vectors can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows\n",
    "\n",
    "In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830b034",
   "metadata": {},
   "source": [
    "##### 10. Make a comparison between:\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a8c23",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "\n",
    "Sequential backward exclusion and sequential forward selection are both feature selection techniques. Sequential backward exclusion starts with all features and iteratively removes the least significant ones until a desired subset is reached. Sequential forward selection starts with an empty set and iteratively adds the most significant features until a desired subset is reached.\n",
    "\n",
    "Filter methods and wrapper methods are two categories of feature selection methods. Filter methods evaluate feature relevance independently of the learning algorithm, while wrapper methods use the learning algorithm's performance as a criterion for feature selection.\n",
    "\n",
    "SMC (Similarity Matching Coefficient) and Jaccard coefficient are similarity measures used for comparing sets. SMC counts the number of matching elements divided by the number of total elements, while the Jaccard coefficient calculates the ratio of the intersection to the union of two sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
