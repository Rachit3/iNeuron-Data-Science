{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455f174",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "The key reasons for reducing the dimensionality of a dataset are to eliminate redundant or irrelevant features, improve computational efficiency, and mitigate the curse of dimensionality.\n",
    "\n",
    "The major disadvantages of reducing dimensionality include potential loss of information, difficulty in selecting the most informative features, and increased risk of underfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is the dimensionality curse ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3d77b",
   "metadata": {},
   "source": [
    "**Ans:** The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cd70e",
   "metadata": {},
   "source": [
    "**Ans:** No, dimensionality reduction is not reversible in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d953e0",
   "metadata": {},
   "source": [
    "**Ans:** It Depends on dataset. If it is comprised of points that are perfectly aligned, PCA can reduce the dataset down to 1 dimension and preserve 95% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a756ac",
   "metadata": {},
   "source": [
    "**Ans:** If I perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. In this case roughly 950 dimensions are required to preserve 95% of the variance. So the answer is, it depends on the dataset, and it could be any number between 1 and 950."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea77b3",
   "metadata": {},
   "source": [
    "**Ans:** The following are the scenarios where the following are used: \n",
    "- Use vanilla PCA when you have enough memory and want a straightforward implementation for dimensionality reduction.\n",
    "- Use incremental PCA when you have limited memory and need to process data in small batches or online settings.\n",
    "- Use randomized PCA when you have large datasets and want a faster approximation of PCA.\n",
    "- Use kernel PCA when dealing with nonlinear relationships in the data and want to capture complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. How do you assess a dimensionality reduction algorithm's success on your dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ec18f",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "To assess the success of a dimensionality reduction algorithm on your dataset, the following factors can be considered:\n",
    "\n",
    "1. Reconstruction quality: Evaluate how well the algorithm reconstructs the original data from the reduced dimensions. Lower reconstruction error indicates better performance.\n",
    "\n",
    "2. Visualization: Examine the reduced-dimensional data visually to see if it preserves the underlying structure and patterns of the original data.\n",
    "\n",
    "3. Retained variance: Assess how much variance or information is retained after dimensionality reduction. Higher retained variance suggests better preservation of important features.\n",
    "\n",
    "4. Impact on downstream tasks: Evaluate the algorithm's impact on the performance of subsequent tasks, such as classification or clustering. If the reduced dimensions still enable accurate predictions, it indicates success.\n",
    "\n",
    "5. Computational efficiency: Consider the algorithm's speed and resource requirements. A successful dimensionality reduction algorithm should be efficient and scalable to handle large datasets.\n",
    "\n",
    "6. Robustness: Assess the algorithm's performance against noise or outliers in the data. A robust algorithm should handle such variations well and maintain meaningful structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. Is it logical to use two different dimensionality reduction algorithms in a chain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6fa75",
   "metadata": {},
   "source": [
    "**Ans:**Yes, it is logical to use two different dimensionality reduction algorithms in a chain if there are specific requirements or advantages offered by each algorithm that can complement each other in reducing the dimensionality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c03b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
